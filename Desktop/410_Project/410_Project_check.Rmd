---
title: "410 Project"
author: "Ben, Dylan, Emily"
date: "`r Sys.Date()`"
output: pdf_document
---


In this project, we aim to predict the likelihood of Alzheimer's disease diagnosis based on various demographic, health, and behavioral factors. The dataset we are working with contains records of individuals, with variables including age, gender, family history of Alzheimer's, cognitive test scores, lifestyle factors (e.g., smoking, alcohol consumption, physical activity level), and more.

The main objective of this analysis is to identify predictors that significantly influence the chance of being diagnosed with Alzheimer's. We will also explore potential collinearity between predictors, as highly correlated variables can impact the model's performance and interpretation.

```{r setup, include=FALSE}
library(mgcv)
library(dplyr)
library(ggplot2)
library(splines)
library(car)
library(MASS)
library(parallel)
library(knitr)
library(tidyr)
library(boot)
library(DAAG)
library(caret)

setwd("/Users/Bendasilva/Desktop/410_Project")
alzData<-read.csv("alzheimers_prediction_Dataset.csv",header = TRUE,sep=",")
## The conversion
# Step 1: Rename columns to match the simplified names
colnames(alzData)[colnames(alzData) == "Family.History.of.Alzheimer.s"] <- "FamilyHistory"
colnames(alzData)[colnames(alzData) == "Genetic.Risk.Factor..APOE.Îµ4.allele."] <- "GeneticRisk"
colnames(alzData)[colnames(alzData) == "Alzheimer.s.Diagnosis"] <- "AlzheimerDiagnosis"
colnames(alzData)[colnames(alzData) == "Physical.Activity.Level"] <- "PhysicalActivityLevel"
colnames(alzData)[colnames(alzData) == "Smoking.Status"] <- "SmokingStatus"
colnames(alzData)[colnames(alzData) == "Cholesterol.Level"] <- "CholesterolLevel"
colnames(alzData)[colnames(alzData) == "Cognitive.Test.Score"] <- "CognitiveTestScore"
colnames(alzData)[colnames(alzData) == "Sleep.Quality"] <- "SleepQuality"
colnames(alzData)[colnames(alzData) == "Air.Pollution.Exposure"] <- "AirPollutionExposure"
colnames(alzData)[colnames(alzData) == "Marital.Status"] <- "MaritalStatus"
colnames(alzData)[colnames(alzData) == "Social.Engagement.Level"] <- "SocialEngagementLevel"
colnames(alzData)[colnames(alzData) == "Stress.Levels"] <- "StressLevels"
colnames(alzData)[colnames(alzData) == "Education.Level"] <- "EducationLevel"
colnames(alzData)[colnames(alzData) == "Alcohol.Consumption"] <- "AlcoholConsumption"
colnames(alzData)[colnames(alzData) == "Depression.Level"] <- "DepressionLevel"
colnames(alzData)[colnames(alzData) == "Dietary.Habits"] <- "DietaryHabits"
colnames(alzData)[colnames(alzData) == "Employment.Status"] <- "EmploymentStatus"
colnames(alzData)[colnames(alzData) == "Income.Level"] <- "IncomeLevel"
colnames(alzData)[colnames(alzData) == "Urban.vs.Rural.Living"] <- "UrbanvsRuralLiving"

# Step 2: Update factor_vars list to use the new column names
factor_vars <- c("Country", "Gender", "PhysicalActivityLevel", "SmokingStatus",
                 "AlcoholConsumption", "Diabetes", "Hypertension", "CholesterolLevel", 
                 "FamilyHistory", "DepressionLevel", "SleepQuality", "DietaryHabits", 
                 "AirPollutionExposure", "EmploymentStatus", "MaritalStatus", 
                 "GeneticRisk", "SocialEngagementLevel", "IncomeLevel", "StressLevels", 
                 "UrbanvsRuralLiving", "AlzheimerDiagnosis")

# Step 3: Check if all factor variables exist in the data
missing_columns <- setdiff(factor_vars, colnames(alzData))
if(length(missing_columns) > 0) {
  stop(paste("Missing columns:", paste(missing_columns, collapse = ", ")))
}

# Step 4: Convert the specified columns to factors (after renaming)
alzData[factor_vars] <- lapply(alzData[factor_vars], as.factor)

# Step 5: Convert numeric columns to numeric (ensure you're using the correct column names)
num_vars <- c("Age", "EducationLevel", "BMI", "CognitiveTestScore")  # Updated names
alzData[num_vars] <- lapply(alzData[num_vars], as.numeric)

# Step 6: Create squared terms (with the updated names)
alzData$Age2 <- alzData$Age^2
alzData$BMI2 <- alzData$BMI^2
alzData$CognitiveTestScore2 <- alzData$CognitiveTestScore^2
alzData$EducationLevel2 <- alzData$EducationLevel^2

# Step 7: Handle categorical variables and factor conversion for categorical columns
alzData[sapply(alzData, is.character)] <- lapply(alzData[sapply(alzData, is.character)], as.factor)
# Order matters for some categorical variables
alzData$AlcoholConsumption <- factor(alzData$AlcoholConsumption, order = TRUE, levels = c("Never", "Occasionally", "Regularly"))
alzData$SmokingStatus <- factor(alzData$SmokingStatus, order = TRUE, levels = c("Never", "Former", "Current"))
# Ensure that numeric columns are converted to numeric
alzData[sapply(alzData, is.numeric)] <- lapply(alzData[sapply(alzData, is.numeric)], as.numeric)
```

## Simple findings from data:

```{r simple plot,echo=FALSE, fig.width=3.5, ,fig.height=3.5,fig.show='hold'}
plot(alzData$AlzheimerDiagnosis,main="Diagnosis variable proportion")
summary(alzData)
```


### First, we identify what variables may have a significant relationship with a positive (or negative) alzheimer's diagnosis:

Introduce logistic regression method (mentioned in class), briefly explain what it is, what a general model looks like (in latex), and why we chose it.

Started with a saturated model. Discuss saturated model results.

Introduce what VIF is, find a textbook as a citation on this. Interpret VIF results.

```{r Full.Log.Reg, echo=FALSE}
# Logistic Regression model on all variables

alz_logit_full <- glm(AlzheimerDiagnosis~ .
                  ,data = alzData
                  ,family = binomial)
sum(vif(alz_logit_full) >= 5) #Check if any vifs are >= 5.
#summary(alz_logit_full)
full.model.aic <- alz_logit_full$aic
# Country, Age, FamilyHistory, and GeneticRisk seem to be the only significant variables.
```

Introduce step-wise feature selection methods (both, backward, forward), and explain why we used this method.

Step-wise method of variable selection suggests Country, Age, FamilyHistory, GeneticRisk, and UrbanvsRuralLiving variables provide the "best" model. However, UrbanvsRuralLiving does not appear to be that significant according to the p-value. Deviance testing will be done to see if this variable can be taken out without significantly affecting model performance.


```{r Step.fwd, echo=FALSE}
#| cache: true
alz_logit_int <- glm(AlzheimerDiagnosis~1,data=alzData,family="binomial")
 
# Step-wise feature selection ("forward" direction)
step.fwd.model<- step(alz_logit_int, direction = "forward", 
                      scope = list(lower = alz_logit_int, upper = alz_logit_full),
                      trace = F) 
vif(step.fwd.model) # Check if any vifs exceed 5 in forward selection model. None exceed 2
#summary(step.fwd.model)
#Forward selection method also yields same model as step-both method and backward selection. This method was much faster than the other two so further analysis will only use this model to represent the best "feature-selection-method" model.
```

Introduce deviance testing method. What is it, what does it look like, why I am using it, what are my results.

Deviance Test: Compare the best feature selected reduced model to one-variable removed models. If removing a variable results in a very large increase in deviance (and a very small p-value), it suggests that the variable plays an important role in the model fit.

The purpose of these tests is to validate if the feature selection method(s) would provide the optimal model.

Results of deviance tests suggest that removing the Urban vs Rural Living variable from the model would not significantly affect the model fit supported by its p-value being 0.096, which is greater than 0.05.

Conclusions made from test: the best reduced model from feature selection methods could be further simplified without significantly affecting the model.

```{r Step.fwd.deviance.tests, echo=FALSE}
## Fit reduced models (each with one variable removed)

no_country.model <- glm(AlzheimerDiagnosis ~ 
                          Age + FamilyHistory + GeneticRisk + UrbanvsRuralLiving, 
                        data = alzData, family = binomial)

no_age.model <- glm(AlzheimerDiagnosis ~ 
                      Country + FamilyHistory + GeneticRisk + UrbanvsRuralLiving,
                    data = alzData, family = binomial)

no_famHist.model <- glm(AlzheimerDiagnosis ~ 
                          Country + Age + GeneticRisk + UrbanvsRuralLiving, 
                        data = alzData, family = binomial)

no_genRisk.model <- glm(AlzheimerDiagnosis ~ 
                          Country + Age + FamilyHistory + UrbanvsRuralLiving, 
                        data = alzData, family = binomial)

no_urbanvsRural.model <- glm(AlzheimerDiagnosis ~ 
                          Country + Age + FamilyHistory + GeneticRisk, 
                        data = alzData, family = binomial)


## Deviance test: Compare the best feature selected reduced model to one-variable removed models
## If removing a variable results in a very large increase in deviance (and a very small p-value), it suggests that the variable plays an important role in the model fit.
sig.no_country <- pchisq(no_country.model$deviance-step.fwd.model$deviance,
                         df=no_country.model$df.residual-step.fwd.model$df.residual,
                         lower.tail = FALSE)

sig.no_age <- pchisq(no_age.model$deviance-step.fwd.model$deviance,
                     df=no_age.model$df.residual-step.fwd.model$df.residual,
                     lower.tail = FALSE)

sig.no_famHist <- pchisq(no_famHist.model$deviance-step.fwd.model$deviance,
                         df=no_famHist.model$df.residual-step.fwd.model$df.residual,
                         lower.tail = FALSE)

sig.no_genRisk <- pchisq(no_genRisk.model$deviance-step.fwd.model$deviance,
                         df=no_genRisk.model$df.residual-step.fwd.model$df.residual,
                         lower.tail = FALSE)

sig.no_urbanvsRural <- pchisq(no_urbanvsRural.model$deviance-step.fwd.model$deviance,
                         df=no_urbanvsRural.model$df.residual-step.fwd.model$df.residual,
                         lower.tail = FALSE)

# Create a table of results
results <- data.frame(
  `Variable Removed` = c("Country", "Age", "Family History", "Genetic Risk", "Urban vs Rural Living"),
  `Degree Freedom Difference` = c(no_country.model$df.residual-step.fwd.model$df.residual,
                             no_age.model$df.residual-step.fwd.model$df.residual,
                             no_famHist.model$df.residual-step.fwd.model$df.residual,
                             no_genRisk.model$df.residual-step.fwd.model$df.residual,
                             no_urbanvsRural.model$df.residual-step.fwd.model$df.residual),
  `P-value` = c(sig.no_country, sig.no_age, sig.no_famHist, sig.no_genRisk, sig.no_urbanvsRural)
)

#Title = "Significance tests for the reduced model"
kable(results)
```

Fitting suggested reduced model and exploring it's results.

The Q-Q residuals of this model are skewed to the right. Since the only variable that is not categorical is Age, we fit a GAM model with splines on Age

```{r Reduced.model.check, echo=FALSE}
## Fit reduced model
alz_logit_reduced <- glm(AlzheimerDiagnosis~ 
                   Country+ Age+ FamilyHistory+ GeneticRisk
                  ,data = alzData
                  ,family = binomial)
#summary(alz_logit_reduced)
alz_logit_red.aic <- alz_logit_reduced$aic
alz_logit_red.aic
plot(alz_logit_reduced, which = 2)
```

Introduce GAM model with splines method. What is it, what does a general GAM method look like (in latex), might also have to give a blurb about splines and how they fit in GAM models.

Fitting a GAM model and putting splines on Age since it's the only variable that is not categorical. Discuss results.

Splines on Age show high significance according to p-value. Gam check plots shows bimodal residuals, which eliminated the skewness from the previous model

```{r GAM.model, echo=FALSE}
alz_logit_gam<- gam(AlzheimerDiagnosis~ 
                   Country+ s(Age)+ FamilyHistory+ GeneticRisk
                  ,data = alzData
                  ,family = binomial)
summary(alz_logit_gam)
# Splines on Age show high significance according to p-value.
alz_logit_gam.aic <- alz_logit_gam$aic
alz_logit_gam.aic

gam.check(alz_logit_gam)
#gam check plots shows bimodal, normal residuals, which eliminated the skewness from the previous model
```

Introduce log odds method; what is it (show a formula in latex), why are we using it, interpret our results. Cite a textbook.


```{r Log.odds, echo=FALSE}
#Calculate log-odds ratios for the reduced models to compare the magnitudes across predictors to see which ones have the strongest change in odds
alz_reduced.log_odds_ratio <- exp(coef(alz_logit_reduced))
alz_reduced.log_odds_ratio

alz_gam.log_odds_ratio <- exp(coef(alz_logit_gam))
alz_gam.log_odds_ratio
#Interpret the log odds ratio of the 2 models by comparing the magnitudes. Put into a table for clarity, sorted desc

# Convert to data frames
df_reduced <- data.frame(Predictor = names(alz_reduced.log_odds_ratio), LogOdds_Reduced = alz_reduced.log_odds_ratio)
df_gam <- data.frame(Predictor = names(alz_gam.log_odds_ratio), LogOdds_GAM = alz_gam.log_odds_ratio)

# Merge both data frames by Predictor (outer join to keep all predictors)
log_odds_table <- full_join(df_reduced, df_gam, by = "Predictor")

# Sort by the strongest log-odds in the reduced model
log_odds_table <- log_odds_table %>% arrange(desc(abs(LogOdds_GAM)))

# Display table
kable(log_odds_table, caption = "Comparison of Log-Odds Ratios Between Reduced Models")
```


Introduce AIC, what is it, what does its formula look like, why is it used and how are we using it.

## In order to compare against Emily's quadratic and interaction models, we will compare AIC's of the different models. Start a table with AIC values of my models. Will add Emily's models and AIC's when we combine everything. We will use this to ultimately determine the best model.

## For final report, for each method: include what a method is and if not mentioned in class - a citation from some textbook that the method is found, how it is done (latex of formula or process), reasoning for the method, and discuss the results.


So digging into predictors individually, we look at ages relationship with the aggregate of "yes" diagnosis:

```{r AGE_diagnosis_plot, echo=FALSE}
# Count the number of 'Yes' diagnoses and total samples per age
alzData_countage <- alzData %>%
  group_by(Age) %>%
  summarise(
    Yes_Count = sum(AlzheimerDiagnosis == "Yes", na.rm = TRUE),
    Total_Sample_Size = n()
  )
# Plot
ggplot(alzData_countage, aes(x = Age)) +
  geom_bar(aes(y = Yes_Count), stat = "identity", fill = "blue", alpha = 0.6) +  # Blue bar for 'Yes' counts
  geom_bar(aes(y = Total_Sample_Size), stat = "identity", fill = "red", alpha = 0.3) +  # Red bar for total sample size
  labs(title = "Count of 'Yes' Alzheimer's Diagnosis and Total Sample Size by Age",
       x = "Age",
       y = "Count / Sample Size") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))  # Rotate x-axis labels for readability
```
We can find this almost step wise trend relating to total diagnosis's counted for each age group (50-65, 65-75,75-100)

```{r step-function,, fig.height=3.5, fig.width=3.5,fig.show='hold',echo=FALSE}
table(cut(alzData_countage$Age, breaks = c(min(alzData_countage$Age), 65, 75, max(alzData_countage$Age))))
alzData_countage$Age_Cut <- cut(alzData_countage$Age, breaks = c(min(alzData_countage$Age), 65, 75, max(alzData_countage$Age)))
fit_step <- lm(Yes_Count ~ Age_Cut, data = alzData_countage)
print(coef(summary(fit_step)))
agelims<-range(alzData_countage$Age)
age_grid = seq(from = min(agelims), to = max(agelims))
# Predict the value of the generated ages, returning the standard error using se = TRUE
preds = predict(fit_step, newdata = alzData_countage, se = TRUE)
# Compute error bands (2*SE)
se_bands = cbind("upper" = preds$fit+2*preds$se.fit, 
                 "lower" = preds$fit-2*preds$se.fit)
# Plot
ggplot() +
  geom_point(data = alzData_countage, aes(x = Age, y = Yes_Count)) +
  geom_line(aes(x = age_grid, y = preds$fit), color = "#0000FF") +
  geom_ribbon(aes(x = age_grid, 
                  ymin = se_bands[,"lower"], 
                  ymax = se_bands[,"upper"]), 
              alpha = 0.3) +
  xlim(agelims) +
  labs(title = "Step Function")
hist(residuals(fit_step), main = "Histogram of Residuals", breaks = 20)
qqnorm(residuals(fit_step))
qqline(residuals(fit_step), col = "red")
#plot(fit_step)
```

groups: 50-65, 65-75,75-100

```{r Bspline, fig.height=3.5, fig.width=3.5,fig.show='hold', echo=FALSE}
# spline model, deg 1 and 3
bsplineProj<-lm(alzData_countage$Yes_Count~bs(alzData_countage$Age,knots=c( 65,75),degree=1),data=alzData_countage)
bsplineProjDeg3<-lm(alzData_countage$Yes_Count~bs(alzData_countage$Age,knots=c( 65,75),degree=3),data=alzData_countage)
plot(alzData_countage$Yes_Count~alzData_countage$Age, data = alzData_countage,pch=16,main="bspline with knots 65 and 75")
pred <- predict(bsplineProj, newdata = data.frame(alzData_countage$Age), interval = "prediction")
y_fit <- pred[, "fit"]
upper_pred <- pred[, "upr"]
lower_pred <- pred[, "lwr"]
with(data.frame(alzData_countage$Age, y_fit), lines(alzData_countage$Age, y_fit, col=4, lwd=2))
with(data.frame(alzData_countage$Age, upper_pred), lines(alzData_countage$Age, upper_pred, col="red", lty=2))
with(data.frame(alzData_countage$Age, lower_pred), lines(alzData_countage$Age, lower_pred, col="red", lty=2))
plot(alzData_countage$Yes_Count~alzData_countage$Age, data = alzData_countage,pch=16,main="bspline with knots 65 and 75, deg 3")
pred1 <- predict(bsplineProjDeg3, newdata = data.frame(alzData_countage$Age), interval = "prediction")
y_fit1 <- pred1[, "fit"]
upper_pred1 <- pred1[, "upr"]
lower_pred1 <- pred1[, "lwr"]
with(data.frame(alzData_countage$Age, y_fit), lines(alzData_countage$Age, y_fit1, col=4, lwd=2))
with(data.frame(alzData_countage$Age, upper_pred1), lines(alzData_countage$Age, upper_pred1, col="red", lty=2))
with(data.frame(alzData_countage$Age, lower_pred1), lines(alzData_countage$Age, lower_pred1, col="red", lty=2))
plot(bsplineProjDeg3)
```

```{r ANOVA, echo=FALSE}
alzData <- alzData %>%
  mutate(AgeCategory = case_when(
    Age >= 50 & Age <= 65 ~ "50-65",
    Age > 65 & Age <= 75 ~ "65-75",
    Age > 75 & Age <= 100 ~ "75-100",
    TRUE ~ "Other"
  ))
# Calculate diagnosis rate (proportion of Yes diagnoses)
alzData <- alzData %>%
  mutate(diagnosis_rate = ifelse(AlzheimerDiagnosis == "Yes", 1, 0))
# Perform ANOVA for diagnosis rate by Country, Gender, FamilyHistory, and AgeCategory
anova_result <- aov(diagnosis_rate ~ Country + Gender + FamilyHistory + AgeCategory, data = alzData)
# Summarize the ANOVA result
print(summary(anova_result))

```

```{r Plot_country_prop, echo=FALSE}
# Calculate the average diagnosis rate for each combination of Country and Age Category
alzData_avg <- alzData %>%
  group_by(Country, AgeCategory) %>%
  summarise(diagnosis_rate = mean(AlzheimerDiagnosis == "Yes", na.rm = TRUE)) %>%
  ungroup()  # Ensure that we don't retain grouping in the data
# Now, create the interaction plot
ggplot(alzData_avg, aes(x = AgeCategory, y = diagnosis_rate, color = Country, group = Country)) +
  geom_point(size = 3) +  # Add points for each diagnosis rate
  geom_line() +  # Add a line to show the trend within each country
  labs(title = "Diagnosis Rate by Age Category for all Countries",
       x = "Age Category", 
       y = "Average Diagnosis Rate") +
  theme_minimal() +  # Clean plot theme
  theme(axis.text.x = element_text(angle = 45, hjust = 1))  # Rotate x-axis labels for readability
```


Now to simplify we display only the top and bottom averages overall.

```{r All Countries plot, echo=FALSE}
# Step 1: Calculate the diagnosis rate for each combination of Country and Age Category
plot(x=alzData$Country,y=alzData$AlzheimerDiagnosis)
alzData_avg <- alzData %>%
  group_by(Country, AgeCategory) %>%
  summarise(diagnosis_rate = mean(AlzheimerDiagnosis == "Yes", na.rm = TRUE)) %>%
  ungroup()

# Step 2: Identify the top and bottom countries based on the average diagnosis rate
country_avg_rates <- alzData_avg %>%
  group_by(Country) %>%
  summarise(avg_diagnosis_rate = mean(diagnosis_rate, na.rm = TRUE)) %>%
  arrange(desc(avg_diagnosis_rate))

# Get top and bottom countries
top_country <- country_avg_rates$Country[1]
bottom_country <- country_avg_rates$Country[nrow(country_avg_rates)]

# Step 3: Filter the data for only the top and bottom countries
alzData_top_bottom <- alzData_avg %>%
  filter(Country %in% c(top_country, bottom_country))

# Step 4: Create the plot for the top and bottom countries
ggplot(alzData_top_bottom, aes(x = AgeCategory, y = diagnosis_rate, color = Country, group = Country)) +
  geom_point(size = 3) +  # Add points for each diagnosis rate
  geom_line() +  # Add a line to show the trend within each country
  labs(title = "Diagnosis Rate by Age Category for Top and Bottom Country",
       x = "Age Category", y = "Average Diagnosis Rate") +
  theme_minimal() +  # Clean plot theme
  theme(axis.text.x = element_text(angle = 45, hjust = 1))  # Rotate x-axis labels for readability

```

```{r Plotofimportant,echo=FALSE}
plot(x=alzData$FamilyHistory,y=alzData$AlzheimerDiagnosis)
ggplot(alzData, aes(x = FamilyHistory, fill = as.factor(AlzheimerDiagnosis))) +
  geom_bar(position = "dodge") +
  labs(title = "Alzheimer Diagnosis by Family History",
       x = "Family History of Alzheimer's",
       y = "Count",
       fill = "Alzheimer Diagnosis") +
  theme_minimal() +
  scale_fill_manual(values = c("No" = "lightblue", "Yes" = "lightcoral"))
# Create a contingency table
alz_table <- table(alzData$FamilyHistory, alzData$AlzheimerDiagnosis)
# Perform Chi-Square Test
chi_test <- chisq.test(alz_table)
# Print results
print(chi_test)
```


## Quadratic and log models:

```{r ChangeVarEmily, echo=FALSE}
data<-alzData
colnames(data)[colnames(data) == "Family.History.of.Alzheimer.s"] <- "FamilyHistory"
colnames(data)[colnames(data) == "Genetic.Risk.Factor..APOE.Îµ4.allele."] <- "GeneticRisk"
colnames(data)[colnames(data) == "Alzheimer.s.Diagnosis"] <- "AlzheimerDiagnosis"
colnames(data)[colnames(data) == "Physical.Activity.Level"] <- "PhysicalActivityLevel"
colnames(data)[colnames(data) == "Smoking.Status"] <- "SmokingStatus"
colnames(data)[colnames(data) == "Cholesterol.Level"] <- "CholesterolLevel"
colnames(data)[colnames(data) == "Cognitive.Test.Score"] <- "CognitiveTestScore"
colnames(data)[colnames(data) == "Sleep.Quality"] <- "SleepQuality"
colnames(data)[colnames(data) == "Air.Pollution.Exposure"] <- "AirPollutionExposure"
colnames(data)[colnames(data) == "Marital.Status"] <- "MaritalStatus"
colnames(data)[colnames(data) == "Social.Engagement.Level"] <- "SocialEngagementLevel"
colnames(data)[colnames(data) == "Stress.Levels"] <- "StressLevels"
colnames(data)[colnames(data) == "Education.Level"] <- "EducationLevel"
colnames(data)[colnames(data) == "Alcohol.Consumption"] <- "AlcoholConsumption"
colnames(data)[colnames(data) == "Depression.Level"] <- "DepressionLevel"
colnames(data)[colnames(data) == "Dietary.Habits"] <- "DietaryHabits"
colnames(data)[colnames(data) == "Employment.Status"] <- "EmploymentStatus"
colnames(data)[colnames(data) == "Income.Level"] <- "IncomeLevel"
colnames(data)[colnames(data) == "Urban.vs.Rural.Living"] <- "UrbanvsRuralLiving"

trans_variables <- c("Country", "Gender", "PhysicalActivityLevel", "SmokingStatus", 
                 "AlcoholConsumption", "Diabetes", "Hypertension", "CholesterolLevel", 
                 "FamilyHistory", "DepressionLevel", "SleepQuality", "DietaryHabits", 
                 "AirPollutionExposure", "EmploymentStatus", "MaritalStatus", 
                 "GeneticRisk", "SocialEngagementLevel", "IncomeLevel", "StressLevels", 
                 "UrbanvsRuralLiving", "AlzheimerDiagnosis")
data[trans_variables] <- lapply(data[trans_variables], as.factor)
trans_num_vars <- c("Age", "EducationLevel", "BMI", "CognitiveTestScore")
data[trans_num_vars] <- lapply(data[trans_num_vars], as.numeric)
data$Age2 <- data$Age^2
data$BMI2 <- data$BMI^2
data$CognitiveTestScore2 <- data$CognitiveTestScore^2
data$EducationLevel2 <- data$EducationLevel^2
```

quadratic

```{r Quad model, echo=FALSE}
quadratic_model <- glm(AlzheimerDiagnosis ~ Age + Age2 + BMI + BMI2 +CognitiveTestScore + CognitiveTestScore2 + EducationLevel + EducationLevel2 +PhysicalActivityLevel + SmokingStatus + AlcoholConsumption + Hypertension + Diabetes + CholesterolLevel + FamilyHistory +DepressionLevel + SleepQuality + DietaryHabits + AirPollutionExposure + EmploymentStatus + MaritalStatus +GeneticRisk + SocialEngagementLevel + IncomeLevel + StressLevels +UrbanvsRuralLiving + Gender, family = binomial, data = data)
AIC(quadratic_model)
BIC(quadratic_model)
```

logistic

```{r logistic Model}
logistic_model <- glm(AlzheimerDiagnosis ~ Age + BMI + CognitiveTestScore + EducationLevel +PhysicalActivityLevel + SmokingStatus + AlcoholConsumption + Hypertension + Diabetes + CholesterolLevel + FamilyHistory +DepressionLevel + SleepQuality + DietaryHabits+ AirPollutionExposure + EmploymentStatus + MaritalStatus +GeneticRisk + SocialEngagementLevel + IncomeLevel + StressLevels + UrbanvsRuralLiving + Gender,family = binomial, data = data)
AIC(logistic_model)
BIC(logistic_model)
```

Based on AIC and BIC, the quadratic model's are lower, so its performance better than logistic

```{r FamilyHist}
data %>%
  group_by(FamilyHistory) %>%
  summarise(rate = mean(AlzheimerDiagnosis == "Yes", na.rm = TRUE))

ggplot(data, aes(x = FamilyHistory, fill = AlzheimerDiagnosis)) +
  geom_bar(position = "fill") +
  ylab("Proportion of Diagnosed") +
  labs(title = "Alzheimer's Diagnosis by Family History")
```

As can be seen from the chart, among those without a Family History, 36.8% were diagnosed with Alzheimer, while among those with a Family History, 51.9% were diagnosed with Alzheimer.Therefore, Family History is an important indicator for judging Alzheimer, and people with a Family History are more likely to suffer from Alzheimer.

```{r GeneticRisk}
data %>%
  group_by(GeneticRisk) %>%
  summarise(rate = mean(AlzheimerDiagnosis == "Yes", na.rm = TRUE))
ggplot(data, aes(x =GeneticRisk, fill = AlzheimerDiagnosis)) +
  geom_bar(position = "fill") +
  ylab("Proportion of Diagnosed") +
  labs(title = "Alzheimer's Diagnosis by Genetic Risk")
```
Among people without genetic risk, the proportion of people diagnosed with Alzheimer's is 36.6%. In some people, this probability is 60.6%. As above, this is also a very important indicator, and people with genetic risk have a higher probability of developing Alzheimer's.

```{r GLM_int, echo=FALSE}
model_af <- glm(AlzheimerDiagnosis ~ Age * FamilyHistory,data = data, family = binomial)
summary(model_af)
AIC(model_af)
cv_model_af <- cv.glm(data, model_af, K = 10)
cv_model_af$delta
```
The Interaction between Age and Family is significant because p-value is 0.012 which is smaller than 0.05

## Gam with CV

```{r GAM, echo=FALSE}
gam_af <- gam(AlzheimerDiagnosis ~ s(Age, by = FamilyHistory, k = 5) + FamilyHistory,data = data, family = binomial)
plot(gam_af, pages = 1, se = TRUE)
AIC(gam_af)
data$AlzheimerDiagnosis <- ifelse(data$AlzheimerDiagnosis == "Yes", 1, 0)
cv_gam_af <- cv.glm(data, gam_af, K = 10)
cv_gam_af$delta
```

No FH:50-60 stable, 60-80 increase sharply; 80+ stable
Yes FH:50-60 higher than NO FH, 60-80 same as NO FH

```{r ModelCompare, echo=FALSE}
model_comparison <- data.frame(
  Model = c("GLM_af", "GAM_af"),
  AIC = c(AIC(model_af), AIC(gam_af)),
  CV = c(cv_model_af$delta[1], cv_gam_af$delta[1])
)
model_comparison


ggplot(data, aes(x = Age, y = AlzheimerDiagnosis, color = FamilyHistory)) +
  geom_point(alpha = 0.1) +
  geom_smooth(method = "glm", method.args = list(family = "binomial")) +
  labs(title = "Diagnosis Probability by Age and Family History")
```

Alzheimer's risk increases with the age. And risk becomes higher with family history. There is evidence of an interaction between age and family history on Alzheimerâs diagnosis

```{r AIC compare, echo=FALSE}
# Create an empty data frame to store AIC values
aic_comparison <- data.frame(Model = character(), AIC = numeric(), stringsAsFactors = FALSE)
# Add AIC values for each model
aic_comparison <- rbind(aic_comparison, data.frame(Model = "GAM Interaction", AIC = AIC(gam_af)))
aic_comparison <- rbind(aic_comparison, data.frame(Model = "GLM Model", AIC = AIC(model_af)))
aic_comparison <- rbind(aic_comparison, data.frame(Model = "logistic model", AIC = AIC(logistic_model)))
aic_comparison <- rbind(aic_comparison, data.frame(Model = "log reduced", AIC = AIC(alz_logit_reduced)))
aic_comparison <- rbind(aic_comparison, data.frame(Model = "quadratic model", AIC = AIC(quadratic_model)))
aic_comparison <- rbind(aic_comparison, data.frame(Model = "Step forward model", AIC = AIC(step.fwd.model)))
aic_comparison <- rbind(aic_comparison, data.frame(Model = "No Age Log", AIC = AIC(no_age.model)))
aic_comparison <- rbind(aic_comparison, data.frame(Model = "No family hist log", AIC = AIC(no_famHist.model)))
aic_comparison <- rbind(aic_comparison, data.frame(Model = "No genRisk log", AIC = AIC(no_genRisk.model)))
aic_comparison <- rbind(aic_comparison, data.frame(Model = "No urban/rural log", AIC = AIC(no_urbanvsRural.model)))
aic_comparison <- rbind(aic_comparison, data.frame(Model = "No country log", AIC = AIC(no_country.model)))
aic_comparison <- rbind(aic_comparison, data.frame(Model = "full log", AIC = AIC(alz_logit_full)))
aic_comparison <- rbind(aic_comparison, data.frame(Model = "Gam log", AIC = AIC(alz_logit_gam)))

# View the AIC comparison table
print(aic_comparison)
```

```{r}
#TODO area
# Create age categories
#TODO add t-tests for genetic risk and family history
#TODO look into rate
#significance test on japan vs india
#significance on age groups
#maybe small explain of aic
#predict based on model
#log odds
#TODO caching chunks, formatting, remove MLR
#Get ride /format summaries
#add dylans confusion matrix
```

Bibliography:

#### https://www.science.smith.edu/~jcrouser/SDS293/labs/lab12-r.html