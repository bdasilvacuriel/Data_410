---
title: "410 Project"
author: "Ben, Dylan, Emily"
date: "`r Sys.Date()`"
output: pdf_document
---

In this project, we aim to predict the likelihood of Alzheimer's disease diagnosis based on various demographic, health, and behavioral factors. The dataset we are working with contains records of individuals, with variables including age, gender, family history of Alzheimer's, cognitive test scores, lifestyle factors (e.g., smoking, alcohol consumption, physical activity level), and more.

The main objective of this analysis is to identify predictors that significantly influence the chance of being diagnosed with Alzheimer's. We will also explore potential collinearity between predictors, as highly correlated variables can impact the model's performance and interpretation.

```{r setup, include=FALSE}
library(mgcv)
library(dplyr)
library(ggplot2)
library(splines)
library(car)
library(MASS)
library(parallel)

setwd("/Users/Bendasilva/Desktop/410_Project")
alzData<-read.csv("alzheimers_prediction_Dataset.csv",header = TRUE,sep=",")
## The conversion
# Step 1: Rename columns to match the simplified names
colnames(alzData)[colnames(alzData) == "Family.History.of.Alzheimer.s"] <- "FamilyHistory"
colnames(alzData)[colnames(alzData) == "Genetic.Risk.Factor..APOE.Îµ4.allele."] <- "GeneticRisk"
colnames(alzData)[colnames(alzData) == "Alzheimer.s.Diagnosis"] <- "AlzheimerDiagnosis"
colnames(alzData)[colnames(alzData) == "Physical.Activity.Level"] <- "PhysicalActivityLevel"
colnames(alzData)[colnames(alzData) == "Smoking.Status"] <- "SmokingStatus"
colnames(alzData)[colnames(alzData) == "Cholesterol.Level"] <- "CholesterolLevel"
colnames(alzData)[colnames(alzData) == "Cognitive.Test.Score"] <- "CognitiveTestScore"
colnames(alzData)[colnames(alzData) == "Sleep.Quality"] <- "SleepQuality"
colnames(alzData)[colnames(alzData) == "Air.Pollution.Exposure"] <- "AirPollutionExposure"
colnames(alzData)[colnames(alzData) == "Marital.Status"] <- "MaritalStatus"
colnames(alzData)[colnames(alzData) == "Social.Engagement.Level"] <- "SocialEngagementLevel"
colnames(alzData)[colnames(alzData) == "Stress.Levels"] <- "StressLevels"
colnames(alzData)[colnames(alzData) == "Education.Level"] <- "EducationLevel"
colnames(alzData)[colnames(alzData) == "Alcohol.Consumption"] <- "AlcoholConsumption"
colnames(alzData)[colnames(alzData) == "Depression.Level"] <- "DepressionLevel"
colnames(alzData)[colnames(alzData) == "Dietary.Habits"] <- "DietaryHabits"
colnames(alzData)[colnames(alzData) == "Employment.Status"] <- "EmploymentStatus"
colnames(alzData)[colnames(alzData) == "Income.Level"] <- "IncomeLevel"
colnames(alzData)[colnames(alzData) == "Urban.vs.Rural.Living"] <- "UrbanvsRuralLiving"

# Step 2: Update factor_vars list to use the new column names
factor_vars <- c("Country", "Gender", "PhysicalActivityLevel", "SmokingStatus",
                 "AlcoholConsumption", "Diabetes", "Hypertension", "CholesterolLevel", 
                 "FamilyHistory", "DepressionLevel", "SleepQuality", "DietaryHabits", 
                 "AirPollutionExposure", "EmploymentStatus", "MaritalStatus", 
                 "GeneticRisk", "SocialEngagementLevel", "IncomeLevel", "StressLevels", 
                 "UrbanvsRuralLiving", "AlzheimerDiagnosis")

# Step 3: Check if all factor variables exist in the data
missing_columns <- setdiff(factor_vars, colnames(alzData))
if(length(missing_columns) > 0) {
  stop(paste("Missing columns:", paste(missing_columns, collapse = ", ")))
}

# Step 4: Convert the specified columns to factors (after renaming)
alzData[factor_vars] <- lapply(alzData[factor_vars], as.factor)

# Step 5: Convert numeric columns to numeric (ensure you're using the correct column names)
num_vars <- c("Age", "EducationLevel", "BMI", "CognitiveTestScore")  # Updated names
alzData[num_vars] <- lapply(alzData[num_vars], as.numeric)

# Step 6: Create squared terms (with the updated names)
alzData$Age2 <- alzData$Age^2
alzData$BMI2 <- alzData$BMI^2
alzData$CognitiveTestScore2 <- alzData$CognitiveTestScore^2
alzData$EducationLevel2 <- alzData$EducationLevel^2

# Step 7: Handle categorical variables and factor conversion for categorical columns
alzData[sapply(alzData, is.character)] <- lapply(alzData[sapply(alzData, is.character)], as.factor)

# Order matters for some categorical variables
alzData$AlcoholConsumption <- factor(alzData$AlcoholConsumption, order = TRUE, levels = c("Never", "Occasionally", "Regularly"))
alzData$SmokingStatus <- factor(alzData$SmokingStatus, order = TRUE, levels = c("Never", "Former", "Current"))

# Ensure that numeric columns are converted to numeric
alzData[sapply(alzData, is.numeric)] <- lapply(alzData[sapply(alzData, is.numeric)], as.numeric)


```

## Simple findings from data:


```{r simple plot,echo=FALSE, fig.width=3.5, ,fig.height=3.5,fig.show='hold'}
plot(alzData$AlzheimerDiagnosis,main="Diagnosis variable proportion")
summary(alzData)
```

### First, we identify what variables may have a significant relationship with a positive (or negative) alzheimer's diagnosis:


```{r log_model_1,include=FALSE}
# Logistic Regression model on all variables

alz_logit <- glm(AlzheimerDiagnosis~ .
                  ,data = alzData
                  ,family = binomial)
sum(vif(alz_logit) >= 5) #Check if any vifs are >= 5
summary(alz_logit)
```
```{r plotlogbad}
# Predict the probabilities
predictions <- predict(alz_logit, type = "response", se.fit = TRUE)

# Calculate the confidence interval for predictions (95% confidence level)
conf_int_lower <- predictions$fit - 1.96 * predictions$se.fit
conf_int_upper <- predictions$fit + 1.96 * predictions$se.fit

# Create a data frame with the actual data and the predicted values
prediction_df <- data.frame(
  Actual = alzData$AlzheimerDiagnosis,
  Predicted = predictions$fit,
  ConfIntLower = conf_int_lower,
  ConfIntUpper = conf_int_upper
)

# Plot the predicted probabilities with confidence bands
ggplot(prediction_df, aes(x = Predicted, y = Actual)) +
  geom_point(aes(color = Actual), alpha = 0.5) +
  geom_ribbon(aes(ymin = ConfIntLower, ymax = ConfIntUpper), fill = "blue", alpha = 0.2) +
  theme_minimal() +
  labs(title = "Logistic Regression Model Predictions with Confidence Bands",
       x = "Predicted Probability",
       y = "Actual Diagnosis (0 or 1)") +
  theme(legend.position = "none")

# Display AIC of the model
AIC(alz_logit)


```

### Findings from full logistic model:

This model highlights : Country, Age, FamilyHistory, and GeneticRisk as the only significant variables. However, because we have a large number of factor variables, the intercept only model, which contains all the "base values" for each variable is difficult to interpret.

### Now well use a stepwise glm to find the "best" subsets of predictors:

### but let's speed that up with some paralel computing:


```{r bestsub,include=FALSE}
#The following call takes about 8 minutes to run!
#alzBestSubset <- step(alz_logit, direction = "both", trace = F) 
```

Using the best subset from the previous models, we create a logistic model using said variables:

```{r removerural,include=FALSE}
#alzBestSubset[[1]]
#sum(vif(alzBestSubset) >= 5) # Again, check if any vifs exceed 5
#summary(alzBestSubset)  # Although it appears in in the best subset model, 
  # Urban vs Rural Living is really not significant enough to be in there. 
  # Manually remove it.

#alzBestSubset <- subset(alzBestSubset, select = -UrbanvsRuralLiving)
  
  #PROF JOHN THINKS: maybe try fixing all but one variable and see the effects of the changing variable (how to do this?)
```

# Logistic Regression model on genetic risk and family history and their interaction

```{r better_log, echo=FALSE}
alz_logit2 <- glm(AlzheimerDiagnosis~ 
                  FamilyHistory+GeneticRisk
                  +FamilyHistory*GeneticRisk
                  ,data = alzData
                  ,family = binomial)
summary(alz_logit2)
```

```{r betterlog_plot, echo=FALSE}
# Predict the probabilities for the new model
predictions2 <- predict(alz_logit2, type = "response", se.fit = TRUE)

# Calculate the confidence interval for predictions (95% confidence level)
conf_int_lower2 <- predictions2$fit - 1.96 * predictions2$se.fit
conf_int_upper2 <- predictions2$fit + 1.96 * predictions2$se.fit

# Create a data frame with the actual data and the predicted values for the second model
prediction_df2 <- data.frame(
  Actual = alzData$AlzheimerDiagnosis,
  Predicted = predictions2$fit,
  ConfIntLower = conf_int_lower2,
  ConfIntUpper = conf_int_upper2
)

# Plot the predicted probabilities with confidence bands for the second model
ggplot(prediction_df2, aes(x = Predicted, y = Actual)) +
  geom_point(aes(color = Actual), alpha = 0.5) +
  geom_ribbon(aes(ymin = ConfIntLower, ymax = ConfIntUpper), fill = "blue", alpha = 0.2) +
  theme_minimal() +
  labs(title = "Logistic Regression Model 2 Predictions with Confidence Bands",
       x = "Predicted Probability",
       y = "Actual Diagnosis (0 or 1)") +
  theme(legend.position = "none")

# Display AIC of the second model
AIC(alz_logit2)
```


#### The interaction term is not significant.

# MLR model below:

```{r MLR, echo=FALSE}
alz_logitlm <- lm(AlzheimerDiagnosis~ 
                  FamilyHistory+GeneticRisk
                  +FamilyHistory*GeneticRisk
                  ,data = alzData
                  )
#summary(alz_logitlm)
#plot(alz_logitlm)

# Get predictions from the linear regression model
predictions_lm <- predict(alz_logitlm)

# Create a data frame with the actual data and predicted values for plotting
prediction_df_lm <- data.frame(
  Actual = alzData$AlzheimerDiagnosis,
  Predicted = predictions_lm
)

# Plot actual vs predicted values for the linear model
ggplot(prediction_df_lm, aes(x = Predicted, y = Actual)) +
  geom_point(color = "blue", alpha = 0.5) +
  geom_smooth(method = "lm", color = "red", se = FALSE) +
  theme_minimal() +
  labs(title = "Linear Regression Model: Predicted vs Actual Values",
       x = "Predicted Values", 
       y = "Actual Values") +
  theme(legend.position = "none")

# Get AIC for the linear regression model
AIC(alz_logitlm)
```

So digging into predictors individually, we look at ages relationship with the aggregate of "yes" diagnosis:

```{r AGE_diagnosis_plot, echo=FALSE}
# Count the number of 'Yes' diagnoses and total samples per age
alzData_countage <- alzData %>%
  group_by(Age) %>%
  summarise(
    Yes_Count = sum(AlzheimerDiagnosis == "Yes", na.rm = TRUE),
    Total_Sample_Size = n()
  )
# Plot
ggplot(alzData_countage, aes(x = Age)) +
  geom_bar(aes(y = Yes_Count), stat = "identity", fill = "blue", alpha = 0.6) +  # Blue bar for 'Yes' counts
  geom_bar(aes(y = Total_Sample_Size), stat = "identity", fill = "red", alpha = 0.3) +  # Red bar for total sample size
  labs(title = "Count of 'Yes' Alzheimer's Diagnosis and Total Sample Size by Age",
       x = "Age",
       y = "Count / Sample Size") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))  # Rotate x-axis labels for readability

```
We can find this almost step wise trend relating to total diagnosis's counted for each age group (50-65, 65-75,75-100)

```{r step-function,, fig.height=3.5, fig.width=3.5,fig.show='hold',echo=FALSE}
table(cut(alzData_countage$Age, breaks = c(min(alzData_countage$Age), 65, 75, max(alzData_countage$Age))))
alzData_countage$Age_Cut <- cut(alzData_countage$Age, breaks = c(min(alzData_countage$Age), 65, 75, max(alzData_countage$Age)))
fit_step <- lm(Yes_Count ~ Age_Cut, data = alzData_countage)

print(coef(summary(fit_step)))

agelims<-range(alzData_countage$Age)
age_grid = seq(from = min(agelims), to = max(agelims))

# Predict the value of the generated ages, returning the standard error using se = TRUE
preds = predict(fit_step, newdata = alzData_countage, se = TRUE)

# Compute error bands (2*SE)
se_bands = cbind("upper" = preds$fit+2*preds$se.fit, 
                 "lower" = preds$fit-2*preds$se.fit)

# Plot
ggplot() +
  geom_point(data = alzData_countage, aes(x = Age, y = Yes_Count)) +
  geom_line(aes(x = age_grid, y = preds$fit), color = "#0000FF") +
  geom_ribbon(aes(x = age_grid, 
                  ymin = se_bands[,"lower"], 
                  ymax = se_bands[,"upper"]), 
              alpha = 0.3) +
  xlim(agelims) +
  labs(title = "Step Function")

hist(residuals(fit_step), main = "Histogram of Residuals", breaks = 20)

qqnorm(residuals(fit_step))
qqline(residuals(fit_step), col = "red")
plot(fit_step)

```

50-65, 65-75,75-100

```{r Bspline, fig.height=3.5, fig.width=3.5,fig.show='hold', echo=FALSE}
# spline model
bsplineProj<-lm(alzData_countage$Yes_Count~bs(alzData_countage$Age,knots=c( 65,75),degree=1),data=alzData_countage)

bsplineProjDeg3<-lm(alzData_countage$Yes_Count~bs(alzData_countage$Age,knots=c( 65,75),degree=3),data=alzData_countage)


plot(alzData_countage$Yes_Count~alzData_countage$Age, data = alzData_countage,pch=16,main="bspline with knots 65 and 75")

pred <- predict(bsplineProj, newdata = data.frame(alzData_countage$Age), interval = "prediction")

y_fit <- pred[, "fit"]
upper_pred <- pred[, "upr"]
lower_pred <- pred[, "lwr"]

with(data.frame(alzData_countage$Age, y_fit), lines(alzData_countage$Age, y_fit, col=4, lwd=2))

with(data.frame(alzData_countage$Age, upper_pred), lines(alzData_countage$Age, upper_pred, col="red", lty=2))
with(data.frame(alzData_countage$Age, lower_pred), lines(alzData_countage$Age, lower_pred, col="red", lty=2))

##start
plot(alzData_countage$Yes_Count~alzData_countage$Age, data = alzData_countage,pch=16,main="bspline with knots 65 and 75, deg 3")

pred1 <- predict(bsplineProjDeg3, newdata = data.frame(alzData_countage$Age), interval = "prediction")

y_fit1 <- pred1[, "fit"]
upper_pred1 <- pred1[, "upr"]
lower_pred1 <- pred1[, "lwr"]

with(data.frame(alzData_countage$Age, y_fit), lines(alzData_countage$Age, y_fit1, col=4, lwd=2))

with(data.frame(alzData_countage$Age, upper_pred1), lines(alzData_countage$Age, upper_pred1, col="red", lty=2))
with(data.frame(alzData_countage$Age, lower_pred1), lines(alzData_countage$Age, lower_pred1, col="red", lty=2))
plot(bsplineProjDeg3)

```


```{r ANOVA, echo=FALSE}
# Create age categories
alzData <- alzData %>%
  mutate(AgeCategory = case_when(
    Age >= 50 & Age <= 65 ~ "50-65",
    Age > 65 & Age <= 75 ~ "65-75",
    Age > 75 & Age <= 100 ~ "75-100",
    TRUE ~ "Other"
  ))

# Calculate diagnosis rate (proportion of Yes diagnoses)
alzData <- alzData %>%
  mutate(diagnosis_rate = ifelse(AlzheimerDiagnosis == "Yes", 1, 0))

# Perform ANOVA for diagnosis rate by Country, Gender, FamilyHistory, and AgeCategory
anova_result <- aov(diagnosis_rate ~ Country + Gender + FamilyHistory + AgeCategory, data = alzData)

# Summarize the ANOVA result
summary(anova_result)

# Calculate the average diagnosis rate for each combination of Country and Age Category
alzData_avg <- alzData %>%
  group_by(Country, AgeCategory) %>%
  summarise(diagnosis_rate = mean(AlzheimerDiagnosis == "Yes", na.rm = TRUE)) %>%
  ungroup()  # Ensure that we don't retain grouping in the data

# Now, create the interaction plot
ggplot(alzData_avg, aes(x = AgeCategory, y = diagnosis_rate, color = Country, group = Country)) +
  geom_point(size = 3) +  # Add points for each diagnosis rate
  geom_line() +  # Add a line to show the trend within each country
  labs(title = "Diagnosis Rate by Age Category for Top 5 Countries",
       x = "Age Category", 
       y = "Average Diagnosis Rate") +
  theme_minimal() +  # Clean plot theme
  theme(axis.text.x = element_text(angle = 45, hjust = 1))  # Rotate x-axis labels for readability

```

```{r All Countries plot, echo=FALSE}
# Step 1: Calculate the diagnosis rate for each combination of Country and Age Category
alzData_avg <- alzData %>%
  group_by(Country, AgeCategory) %>%
  summarise(diagnosis_rate = mean(AlzheimerDiagnosis == "Yes", na.rm = TRUE)) %>%
  ungroup()

# Step 2: Identify the top and bottom countries based on the average diagnosis rate
country_avg_rates <- alzData_avg %>%
  group_by(Country) %>%
  summarise(avg_diagnosis_rate = mean(diagnosis_rate, na.rm = TRUE)) %>%
  arrange(desc(avg_diagnosis_rate))

# Get top and bottom countries
top_country <- country_avg_rates$Country[1]
bottom_country <- country_avg_rates$Country[nrow(country_avg_rates)]

# Step 3: Filter the data for only the top and bottom countries
alzData_top_bottom <- alzData_avg %>%
  filter(Country %in% c(top_country, bottom_country))

# Step 4: Create the plot for the top and bottom countries
ggplot(alzData_top_bottom, aes(x = AgeCategory, y = diagnosis_rate, color = Country, group = Country)) +
  geom_point(size = 3) +  # Add points for each diagnosis rate
  geom_line() +  # Add a line to show the trend within each country
  labs(title = "Diagnosis Rate by Age Category for Top and Bottom Country",
       x = "Age Category", y = "Average Diagnosis Rate") +
  theme_minimal() +  # Clean plot theme
  theme(axis.text.x = element_text(angle = 45, hjust = 1))  # Rotate x-axis labels for readability

```


## Quadratic and log models:

```{r Quadratic log}
# Fit the logistic regression model with squared terms
model <- glm(AlzheimerDiagnosis ~ Age + Age2 + BMI + BMI2 +  
             CognitiveTestScore + CognitiveTestScore2 + EducationLevel + EducationLevel2 +  
             PhysicalActivityLevel + SmokingStatus + AlcoholConsumption +  
             Hypertension + Diabetes + CholesterolLevel + FamilyHistory +  
             DepressionLevel + SleepQuality + DietaryHabits +  
             AirPollutionExposure + EmploymentStatus + MaritalStatus +  
             GeneticRisk + SocialEngagementLevel + IncomeLevel + StressLevels +  
             UrbanvsRuralLiving + Gender,  
             family = binomial, data = alzData)

# Display model information
AIC(model)
BIC(model)
summary(model)

# Logistic regression model without squared terms
model_logistic <- glm(AlzheimerDiagnosis ~ Age + BMI + CognitiveTestScore + EducationLevel +  
                      PhysicalActivityLevel + SmokingStatus + AlcoholConsumption +  
                      Hypertension + Diabetes + CholesterolLevel + FamilyHistory +  
                      DepressionLevel + SleepQuality + DietaryHabits +  
                      AirPollutionExposure + EmploymentStatus + MaritalStatus +  
                      GeneticRisk + SocialEngagementLevel + IncomeLevel + StressLevels +  
                      UrbanvsRuralLiving + Gender,  
                      family = binomial, data = alzData)

# Display model information for logistic regression model
AIC(model_logistic)
BIC(model_logistic)
summary(model_logistic)

pred_data <- alzData
pred_data$Prediction <- predict(model, newdata = pred_data, type = "response")

# Plot the prediction vs Age (example for Age)
ggplot(pred_data, aes(x = Age, y = Prediction)) +
  geom_point(aes(color = AlzheimerDiagnosis), alpha = 0.6) + # Points colored by Alzheimer diagnosis
  geom_smooth(method = "loess", aes(fill = ..se..), alpha = 0.2) + # Confidence bands
  labs(title = "Predicted Probability of Alzheimer's Diagnosis by Age",
       x = "Age", y = "Predicted Probability") +
  theme_minimal()


```

```{r AIC compare}
# Create an empty data frame to store AIC values
aic_comparison <- data.frame(Model = character(), AIC = numeric(), stringsAsFactors = FALSE)
# Add AIC values for each model
aic_comparison <- rbind(aic_comparison, data.frame(Model = "model", AIC = AIC(model)))
aic_comparison <- rbind(aic_comparison, data.frame(Model = "model_logistic", AIC = AIC(model_logistic)))
aic_comparison <- rbind(aic_comparison, data.frame(Model = "alz_logit", AIC = AIC(alz_logit)))
aic_comparison <- rbind(aic_comparison, data.frame(Model = "alz_logit2", AIC = AIC(alz_logit2)))
aic_comparison <- rbind(aic_comparison, data.frame(Model = "alz_logitlm", AIC = AIC(alz_logitlm)))

# View the AIC comparison table
print(aic_comparison)
```



Bibliography: 

# https://www.science.smith.edu/~jcrouser/SDS293/labs/lab12-r.html

