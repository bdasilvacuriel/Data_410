---
title: "410 Project"
author: "Ben, Dylan, Emily"
date: "`r Sys.Date()`"
output: pdf_document
---

```{r setup, include=FALSE}
library(mgcv)
library(dplyr)
library(ggplot2)
library(splines)
library(car)


setwd("/Users/Bendasilva/Desktop/410_Project")
alzData<-read.csv("alzheimers_prediction_Dataset.csv",header = TRUE,sep=",")
## The conversion
alzData[sapply(alzData, is.character)] <- lapply(alzData[sapply(alzData, is.character)], 
                                       as.factor)
# Order matters to some of the categorical variables
alzData$Alcohol.Consumption <- factor(alzData$Alcohol.Consumption, order = TRUE, 
                                    levels=c("Never","Occasionally","Regularly"))
alzData$Smoking.Status <- factor(alzData$Smoking.Status, order = TRUE, 
                                    levels=c("Never","Former","Current"))

alzData[sapply(alzData, is.numeric)] <- lapply(alzData[sapply(alzData, is.numeric)],as.numeric)
```

```{r,include=FALSE}
colnames(alzData)[colnames(alzData) == "Family.History.of.Alzheimer.s"] <- "FamilyHistory"
colnames(alzData)[colnames(alzData) == "Genetic.Risk.Factor..APOE.ε4.allele."] <- "GeneticRisk"
colnames(alzData)[colnames(alzData) == "Alzheimer.s.Diagnosis"] <- "AlzheimerDiagnosis"
colnames(alzData)[colnames(alzData) == "Physical.Activity.Level"] <- "PhysicalActivityLevel"
colnames(alzData)[colnames(alzData) == "Smoking.Status"] <- "SmokingStatus"
colnames(alzData)[colnames(alzData) == "Cholesterol.Level"] <- "CholesterolLevel"
colnames(alzData)[colnames(alzData) == "Cognitive.Test.Score"] <- "CognitiveTestScore"
colnames(alzData)[colnames(alzData) == "Sleep.Quality"] <- "SleepQuality"
colnames(alzData)[colnames(alzData) == "Air.Pollution.Exposure"] <- "AirPollutionExposure"
colnames(alzData)[colnames(alzData) == "Marital.Status"] <- "MaritalStatus"
colnames(alzData)[colnames(alzData) == "Social.Engagement.Level"] <- "SocialEngagementLevel"
colnames(alzData)[colnames(alzData) == "Stress.Levels"] <- "StressLevels"
colnames(alzData)[colnames(alzData) == "Education.Level"] <- "EducationLevel"
colnames(alzData)[colnames(alzData) == "Alcohol.Consumption"] <- "AlcoholConsumption"
colnames(alzData)[colnames(alzData) == "Depression.Level"] <- "DepressionLevel"
colnames(alzData)[colnames(alzData) == "Dietary.Habits"] <- "DietaryHabits"
colnames(alzData)[colnames(alzData) == "Employment.Status"] <- "EmploymentStatus"
colnames(alzData)[colnames(alzData) == "Income.Level"] <- "IncomeLevel"
colnames(alzData)[colnames(alzData) == "Urban.vs.Rural.Living"] <- "UrbanvsRuralLiving"
```

```{r echo=FALSE, fig.width=3.5, ,fig.height=3.5}
plot(alzData$AlzheimerDiagnosis,main="Diagnosis variable proportion")
```

### First, we identify what variables may have a significant relationship with a positive (or negative) alzheimer's diagnosis:


```{r}

# Logistic Regression model on all variables

alz_logit <- glm(AlzheimerDiagnosis~ .
                  ,data = alzData
                  ,family = binomial)
sum(vif(alz_logit) >= 5) #Check if any vifs are >= 5
summary(alz_logit)
```

### Findings from full logistic model:

This model highlights : Country, Age, FamilyHistory, and GeneticRisk as the only significant variables. However, because we have a large number of factor variables, the intercept only model, which contains all the "base values" for each variable is difficult to interpret.

### Now well use a stepwise glm to find the "best" subsets of predictors:

```{r}
#The following call takes about 8 minutes to run!
alzBestSubset <- step(alz_logit, direction = "both", trace = F) 
```

Using the best subset from the previous models, we create a logistic model using said variables:

```{r}
sum(vif(alzBestSubset) >= 5) # Again, check if any vifs exceed 5
summary(alzBestSubset)  # Although it appears in in the best subset model, 
  # Urban vs Rural Living is really not significant enough to be in there. 
  # Manually remove it.

alzBestSubset <- subset(alzBestSubset, select = -UrbanvsRuralLiving)
  
  #PROF JOHN THINKS: maybe try fixing all but one variable and see the effects of the changing variable (how to do this?)
```

# Logistic Regression model on genetic risk and family history and their interaction

```{r}


alz_logit2 <- glm(AlzheimerDiagnosis~ 
                  FamilyHistory+GeneticRisk
                  +FamilyHistory*GeneticRisk
                  ,data = alzData
                  ,family = binomial)
summary(alz_logit2)
```
#### The interaction term is not significant.

# MLR model below:

```{r}
alz_resp <- as.numeric(alzData$AlzheimerDiagnosis)
alz_logitlm <- lm(alz_resp~ 
                  FamilyHistory+GeneticRisk
                  +FamilyHistory*GeneticRisk
                  ,data = alzData
                  )
summary(alz_logitlm)
plot(alz_logitlm)

```

So digging into predictors individually, we look at ages relationship with the aggregate of "yes" diagnosis:

```{r}
# Count the number of 'Yes' diagnoses and total samples per age
alzData_countage <- alzData %>%
  group_by(Age) %>%
  summarise(
    Yes_Count = sum(AlzheimerDiagnosis == "Yes", na.rm = TRUE),
    Total_Sample_Size = n()
  )
# Plot
ggplot(alzData_countage, aes(x = Age)) +
  geom_bar(aes(y = Yes_Count), stat = "identity", fill = "blue", alpha = 0.6) +  # Blue bar for 'Yes' counts
  geom_bar(aes(y = Total_Sample_Size), stat = "identity", fill = "red", alpha = 0.3) +  # Red bar for total sample size
  labs(title = "Count of 'Yes' Alzheimer's Diagnosis and Total Sample Size by Age",
       x = "Age",
       y = "Count / Sample Size") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))  # Rotate x-axis labels for readability

```
We can find this almost step wise trend relating to total diagnosis's counted for each age group (50-60,70,80-90)

#TODO add more splines part

```{r}
table(cut(alzData_countage$Age, breaks = c(min(alzData_countage$Age), 65, 75, max(alzData_countage$Age))))
alzData_countage$Age_Cut <- cut(alzData_countage$Age, breaks = c(min(alzData_countage$Age), 65, 75, max(alzData_countage$Age)))
fit_step <- lm(Yes_Count ~ Age_Cut, data = alzData_countage)

print(coef(summary(fit_step)))

agelims<-range(alzData_countage$Age)
age_grid = seq(from = min(agelims), to = max(agelims))

# Predict the value of the generated ages, returning the standard error using se = TRUE
preds = predict(fit_step, newdata = alzData_countage, se = TRUE)

# Compute error bands (2*SE)
se_bands = cbind("upper" = preds$fit+2*preds$se.fit, 
                 "lower" = preds$fit-2*preds$se.fit)

# Plot
ggplot() +
  geom_point(data = alzData_countage, aes(x = Age, y = Yes_Count)) +
  geom_line(aes(x = age_grid, y = preds$fit), color = "#0000FF") +
  geom_ribbon(aes(x = age_grid, 
                  ymin = se_bands[,"lower"], 
                  ymax = se_bands[,"upper"]), 
              alpha = 0.3) +
  xlim(agelims) +
  labs(title = "Step Function")

hist(residuals(fit_step), main = "Histogram of Residuals", breaks = 20)

qqnorm(residuals(fit_step))
qqline(residuals(fit_step), col = "red")
plot(fit_step)

```

```{r}
# spline model
bsplineProj<-lm(alzData_countage$Yes_Count~bs(alzData_countage$Age,knots=c( 65,75),degree=1),data=alzData_countage)

bsplineProjDeg3<-lm(alzData_countage$Yes_Count~bs(alzData_countage$Age,knots=c( 65,75),degree=3),data=alzData_countage)


plot(alzData_countage$Yes_Count~alzData_countage$Age, data = alzData_countage,pch=16,main="bspline with knots 65 and 75")

pred <- predict(bsplineProj, newdata = data.frame(alzData_countage$Age), interval = "prediction")

y_fit <- pred[, "fit"]
upper_pred <- pred[, "upr"]
lower_pred <- pred[, "lwr"]

with(data.frame(alzData_countage$Age, y_fit), lines(alzData_countage$Age, y_fit, col=4, lwd=2))

with(data.frame(alzData_countage$Age, upper_pred), lines(alzData_countage$Age, upper_pred, col="red", lty=2))
with(data.frame(alzData_countage$Age, lower_pred), lines(alzData_countage$Age, lower_pred, col="red", lty=2))

##start
plot(alzData_countage$Yes_Count~alzData_countage$Age, data = alzData_countage,pch=16,main="bspline with knots 65 and 75, deg 3")

pred1 <- predict(bsplineProjDeg3, newdata = data.frame(alzData_countage$Age), interval = "prediction")

y_fit1 <- pred1[, "fit"]
upper_pred1 <- pred1[, "upr"]
lower_pred1 <- pred1[, "lwr"]

with(data.frame(alzData_countage$Age, y_fit), lines(alzData_countage$Age, y_fit1, col=4, lwd=2))

with(data.frame(alzData_countage$Age, upper_pred1), lines(alzData_countage$Age, upper_pred1, col="red", lty=2))
with(data.frame(alzData_countage$Age, lower_pred1), lines(alzData_countage$Age, lower_pred1, col="red", lty=2))
plot(bsplineProjDeg3)

```


## Quadratic and log models:

```{r}

# Updated factor variables based on your column names
factor_vars <- c("Country", "Gender", "Physical.Activity.Level", "Smoking.Status", 
                 "Alcohol.Consumption", "Diabetes", "Hypertension", "Cholesterol.Level", 
                 "Family.History.of.Alzheimer.s", "Depression.Level", "Sleep.Quality", 
                 "Dietary.Habits", "Air.Pollution.Exposure", "Employment.Status", 
                 "Marital.Status", "Genetic.Risk.Factor..APOE.ε4.allele.", 
                 "Social.Engagement.Level", "Income.Level", "Stress.Levels", 
                 "Urban.vs.Rural.Living", "Alzheimer.s.Diagnosis")

# Convert specified columns to factors
alzData[factor_vars] <- lapply(alzData[factor_vars], as.factor)

# Updated numeric variables based on your column names
num_vars <- c("Age", "Education.Level", "BMI", "Cognitive.Test.Score")

# Convert specified columns to numeric
alzData[num_vars] <- lapply(alzData[num_vars], as.numeric)

# Create squared terms
alzData$Age2 <- alzData$Age^2
alzData$BMI2 <- alzData$BMI^2
alzData$CognitiveTestScore2 <- alzData$Cognitive.Test.Score^2
alzData$EducationLevel2 <- alzData$Education.Level^2

# Fit the logistic regression model
model <- glm(Alzheimer.s.Diagnosis ~ Age + Age2 + BMI + BMI2 +  
             Cognitive.Test.Score + CognitiveTestScore2 + Education.Level + EducationLevel2 +  
             Physical.Activity.Level + Smoking.Status + Alcohol.Consumption +  
             Hypertension + Diabetes + Cholesterol.Level + Family.History.of.Alzheimer.s +  
             Depression.Level + Sleep.Quality + Dietary.Habits +  
             Air.Pollution.Exposure + Employment.Status + Marital.Status +  
             Genetic.Risk.Factor..APOE.ε4.allele. + Social.Engagement.Level + Income.Level + Stress.Levels +  
             Urban.vs.Rural.Living + Gender,  
             family = binomial, data = alzData)

# Display model information
AIC(model)
BIC(model)
summary(model)

# Logistic regression model without squared terms
model_logistic <- glm(Alzheimer.s.Diagnosis ~ Age + BMI + Cognitive.Test.Score + Education.Level +  
                      Physical.Activity.Level + Smoking.Status + Alcohol.Consumption +  
                      Hypertension + Diabetes + Cholesterol.Level + Family.History.of.Alzheimer.s +  
                      Depression.Level + Sleep.Quality + Dietary.Habits +  
                      Air.Pollution.Exposure + Employment.Status + Marital.Status +  
                      Genetic.Risk.Factor..APOE.ε4.allele. + Social.Engagement.Level + Income.Level + Stress.Levels +  
                      Urban.vs.Rural.Living + Gender,  
                      family = binomial, data = alzData)

# Display model information for logistic regression model
AIC(model_logistic)
BIC(model_logistic)
summary(model_logistic)
```

Bibliography: 

# https://www.science.smith.edu/~jcrouser/SDS293/labs/lab12-r.html

